\chapter{Event Reconstruction and Selection}
\label{chapter:reco_select}
\thispagestyle{myheadings}

\graphicspath{{3_Chapter_KLZ_Reconstruction_and_Selection/Figures/}}

KamLAND-ZEN uses detailed simulations defined in KLG4Sim, a GEANT4-based Monte Carlo (MC) simulation software. The MC simulated events are tuned with real calibration events to carefully match the real detector response. Simulated and physical events produce detector responses that are reconstructed to extract higher-level information such as energy and position. The reconstructed event information is used for data selection and spectrum fitting. This chapter discusses the MC simulation and event reconstruction procedures used in KamLAND-ZEN 800.

\section{Analysis Framework}
\subsection{Data Flow}
Figure \ref{fig:dataflow} outlines the data flow in KamLAND-ZEN. PMT signals are digitized in either KamFEE or MoGURA, the two DAQ systems discussed in the previous chapter. The digitized signals are stored in Kinoko Data Format (KDF). KDF files contain trigger information and timestamped, digitized PMT waveforms. KDF files also store run condition information in the header. The EventBuilder collates the waveforms of a single event and stores them in a serial file. A waveform analyzer reconstructs hit time and charge (TQ) information for each of these waveforms. The RTQ files hold the Raw-TQ information for each PMT. Event vertices and visible energy are derived from the RTQ files through their respective reconstruction algorithms. There are secondary reconstructions that are also applied to the RTQ files, such as muon track fitting, flasher vetoes, double pulse fit, and unphysical event selections. The general vector file (GVF) is used for the main physics analyses like the one presented in this thesis.

\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.3]{dataflow.png}
	\caption{Data flow in KamLAND from raw waveforms to analysis variables such as energy, vertex, total hit PMTs, etc. \cite{ozaki_phd}}
	\label{fig:dataflow}
\end{figure}

GVF files contain the following information:
\begin{itemize}
	\item \textbf{run number}
	\item \textbf{event number}
	\item \textbf{TimeStamp} based on DAQ clock time (25 ns for KamFEE, 20 ns for MoGDAQ)
	\item \textbf{unixtime} is the number of seconds since January 1st, 1970 and is used for some run vetoes
	\item \textbf{trigger type} records which trigger was used
	\item \textbf{event vertex and badness} event vertices and a radius from the detector center are saved, along with a vertex fit quality parameter called badness 
	\item \textbf{energy/energy17} visible energies given by the fitter; energy17 is the energy estimate using only 17-inch PMTs
	\item \textbf{TotalChargeID/17/OD} sum of all PMT charges of each PMT type
	\item \textbf{numhit/numhit17} the number of hit PMTs/17-inch PMTs in each event
	\item \textbf{NsumMax} the maximum number of hit PMTs in a single DAQ cycle within each event, a "peak" nhit of the event
	\item \textbf{N200OD} maximum number of simultaneous hit OD PMTs within 200 ns windows
	\item \textbf{muon entrance and direction} muon fitter results are recorded
\end{itemize}

Finally, MoGURA events are associated with muon events acquired in KamFEE DAQ (FBE) and stored in a Muon-Neutron Vector File (MNVF) to search for neutron capture events that occur shortly after muons.

\section{Event Reconstruction}
\subsection{Waveform Analysis}
Each digitized waveform has 128 samples with 1.5 ns sample intervals, corresponding to a waveform digitization window of 192 ns. The waveforms are processed and TQ values are reconstructed using the following procedure:
\begin{itemize}
	\item \textbf{Smoothing} Each waveform is smoothed using a running-average first derivative.
	\item \textbf{Baseline adjustment} The baseline of each PMT is collected at the beginning of each run. This baseline is subtracted from each waveform.
	\item \textbf{Peak finding} Peaks are found with running-averaged 1st, 2nd, and 3rd derivatives.
	\item \textbf{Leading-edge and Trailing-edge tag} A leading-edge is stamped as 10 ns before the peak voltage. The trailing edge is stamped when the waveform returns to baseline. An example of this time-stamping is shown in Figure \ref{fig:waveform_analysis}.
	\item \textbf{Waveform Sum calculation} The waveform is integrated from the leading-edge to the trailing-edge.
\end{itemize}

\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.5]{waveform_analysis.png}
	\caption{An example of waveform analysis from thesis \cite{yoshida_phd}. (left) ADC counts of a real waveform after baseline subtraction. The left cyan line is the leading edge, the center red line is the peak position, and the right dark cyan line is a trailing-edge. (right) Clock calibration example on 25 nsec intervals.}
	\label{fig:waveform_analysis}
\end{figure}

When there are multiple hits in a single PMT waveform, the total charge of the hits and the earliest hit time are returned. This simplified information is used for vertex and energy reconstruction. The multi-photoelectron (multi-pe) information is used for double-pulse fitting and muon shower tagging.

\subsection{PMT Corrections}
\subsection*{Low Gain Problem and HV Reductions}

Since approximately 2011, it has been observed that the gain of some 17-inch PMTs gradually decreased. As the gain of the PMTs fell, this compromised the signal-to-background ratio and PMT waveform quality. It was also observed that the PMTs entered a low impedance state before the gain dropped. An HV current and voltage monitor allows for real-time monitoring of this state. Usually, a simple HV power cycle could recover normal PMT behavior. Since 2016, an automatic HV power cycle mechanism has been implemented to mitigate the low gain problem, but the root cause is still unknown.


Each time the PMTs enter the low impedance state, the HV on that channel was reduced in 50--100 V increments. Over time, some of the channels had their HV reduced by up to 450 V. Figure \ref{fig:lowgain_trend} shows the trend in low gain 17-inch PMTs.

\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.3]{lowgain_trend.png}
	\caption{The trend in the number of low gain 17-inch PMTs, before ZEN-800. The number of low gain channels increased gradually, while the sudden increases are from HV reductions performed since 2017 \cite{ozaki_phd}.}
	\label{fig:lowgain_trend}
\end{figure}

%/ TODO
Note about current low pmt gain analysis.

\subsection*{Bad Channel}
A channel is considered bad if the PMT meets one or more of the following criteria:
\begin{itemize}
	\item PMT pulses less than 0.6\% of the time over all events
	\item PMT pulses below 0.48\% for non-muon events
	\item PMT pulses less than 80\% of the time for high-energy muon events
	\item PMT is missing a waveform more than 10\% of the time
	\item Large discrepancy between the two ATWD hits
	\item High muon charge PMTs. A PMT may read much higher charge ($Q_{detected}$) than the average of its surrounding PMTs ($Q_{expected}$). A run is divided into 100 muon intervals; for each interval the criteria is defined as
	\[\frac{1}{N_{interval}}\sum_{i=1}^{N_{interval}}\left(\frac{1}{N_{muon}}\sum_{j=1}^{N_{muon}}\frac{(Q_{expected}-Q_{detected})^2}{Q_{expected}}\right)>1000\ p.e.\]
\end{itemize}
These bad channels are excluded from event reconstruction and physics analyses.

\subsection*{Dark Hit}
Thermal fluctuations can emit electrons off the photocathode, leading to a PMT hit signal. These "dark hits" are an unavoidable hit-level background in PMT detectors; lowering the detector temperature reduces this effect. The dark hit rates are measured from run to run and are factored into our likelihood-maximizing reconstruction algorithms. The hit rate observed 50-100 ns before the PMT hit time rising edge is taken as the dark rate. Figure \ref{fig:darkrate} shows the PMT hit time distribution and the dark rate window.

\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.4]{darkrate.png}
	\caption{An example pmt hit time distribution from data run 14783, the 50-100 ns leading window is taken to measure the pmt dark hit rate. \cite{li_phd}.}
	\label{fig:darkrate}
\end{figure}

\subsection{Primary Vertex Fitter}
The primary vertex fitter provides a rough estimate of a scintillating event's location. This estimate serves as the input to a more thorough, but complex, secondary fitter. The fit works by constructing a hit time residual distribution:
\begin{equation}
	T_{i}^{emit}=T_i-TOF_i = T_i - \frac{\left|R_i-r_{vertex}\right|}{c_{eff}}
	\label{eq:prim_vertex}
\end{equation}
Here $T_i$ is the hit time of the $i^{th}$ PMT, $TOF_i$ is the time it takes for a scintillation photon to traverse from the vertex position to the $i^{th}$ PMT position, $R_i$ is the PMT position, $r_{vertex}$ is the unknown vertex position to fit for, and $c_{eff}$ is the speed of light in the given medium. By fitting $T_i^{emit}$ to match the standard scintillation time profile, a primary $r_{vertex}$ is produced by the fitter.

\subsection{Secondary Fitter}
The secondary V2 fitter uses the $r_{vertex}$ given by the primary fitter to compute $T_0$ according to the equation \ref{eq:sec_v2}
\begin{equation}
	T_0 = \frac{\sum_i \left(T_i^{pmt}-TOF^{pmt}_i\right)\times Q_i}{\sum_iQ_i}- const.
	\label{eq:sec_v2}
\end{equation}
\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.4]{pmt_dist.png}
	\caption{Probability density function of 17-inch and 20-inch PMT hit times calculated from calibration data. The plot is from \cite{ozaki_phd} and originally from a 2005 calibration dataset.}
	\label{fig:pmt_pdfs}
\end{figure}
This $T_0$ is the charge weighted sum of $T_emit$ from \ref{eq:prim_vertex}. This $T_0$ serves as the universal start point of an event. From this time, each PMT hit time is
\begin{equation}
	\tau(x,y,z,T_0) = T^{pomt}_i-TOF_i^{pmt}-T_0
\end{equation}
Finally, these time-of-flight corrected and centered hit time distributions are used to create probability distributions for the 17 and 20 inch PMTs respectively. These PDFs are shown in Figure \ref{fig:pmt_pdfs}. The likelihood function for an individual PMT is defined as:
\begin{equation}
	\phi_i =\frac{\mu\times f_i(\tau_i)+D_i}{\mu\times C_{17/20}+D_i}
\end{equation}
Here, $\mu$ is the pulse shape determination factor, $D_i$ is the dark hit rate for the $i^{th}$ PMT and $C_{17/20}$ is the normalization constant for the 17 or 20 inch PMTs. The overall log-likelihood is given by the $log(L)=\sum_ilog(\phi_i)$. The log-likelihood is maximized by the Newton-Raphson method, in which the $x,y,z,T_0$ are adjusted to the best-fit values, giving us the V2 reconstructed vertex.

\subsection{Energy Reconstruction}
\label{sec:energy_reco}
Likelihood maximization is also used to reconstruct the energy of an event. A likelihood PDF is constructed using the number of hits, charge, and hit timing.
\subsection*{$N_{hit}$ PDF}
The expectation of the number of photons hitting PMT $i$, $\mu_i$, is a function of the visible energy and dark charge.
\begin{equation}
	\mu_i = a_i(x,y,z)\times E_{vis} +d_i
\end{equation}
Here, $a_i(x,y,z)$ is a coefficient that converts the event energy to the number of photons, which is calibrated with neutron events. It is determined by the PMT position $x,y,z$. $d_i$ is the dark noise charge of PMT $i$, which is electronically measured. The probability that $\mu_i$ photons hit the $i$th PMT $j$ times, $k_{ij}$, is ideally expressed by the Poisson distribution:
\begin{equation}
	k_{ij}=\frac{(\mu_i)^j}{j!}e^{-\mu_i}
\end{equation}

However, in KamLAND waveform analysis, the 1 p.e. detection efficiency is reduced by the 0.3 p.e. software charge threshold. This threshold is set to reduce the acceptance of dark noise but also decreases hit detection efficiency. As a result, the PMT hit probability is reduced to:
\begin{equation}
	P_{hit}=1-v_ie^{-\mu_i}
\end{equation}

\subsection*{Hit Charge PDF}
A Gaussian distribution is assumed for the hit charge PDF of each PMT:
\begin{equation}
	f_{i,j(q_i)}=\frac{1}{\sqrt{2\pi j\sigma^2}}exp(-\frac{(q_i-j)^2}{2j\sigma^2})
\end{equation}
$q_i$ is the observed charge in p.e. units and $\sigma$ is  the charge resolution against 1 p.e. distribution.

\subsection*{Hit Time PDF}
PMT hit timing factors into energy reconstruction by helping to discriminate hits unrelated to the physical event. The hit timing model is created using source calibration data. 
\begin{equation}
	P_{time,i} = \frac{\psi(t_i)a_i E_{vis}+d_i}{\mu_i}
\end{equation}
The PDF is the sum of the signal hit distribution and the constant dark noise.
\subsection*{Energy Likelihood}
The likelihood function to be maximized is constructed as 
\begin{equation}
	L=\prod_{Not\ hit\ PMTs}P_{no-hit,i}\prod_{Hit\ PMTs}\left[P_{hit,i}\left(\sum_{j=1}^{100}f_{i,j}\right)P_{time, i}\right]
	\label{eq:energy_likelihood}
\end{equation}
The reconstructed energy is the one which maximizes this likelihood. The Newton-Raphson method is used to search for this energy. This process is implemented independently for the 17-inch PMTs and 20-inch PMTs, then the event energy is calculated with a weighting factor $\alpha$:
\begin{equation}
	E_{vis}=(1-\alpha)E_{17inch}+\alpha E_{20inch}
\end{equation}
The weighting factor $\alpha=0.3$ was determined to maximize energy resolution.

\subsection*{Bad Channels in Energy Reconstruction}
The increase in the number of low gain PMTs has lead to worsening energy resolution over time, as these PMTs are excluded from the typical energy reconstruction described above. In particular, some of the low gain PMTs still detect photons, but proper gain calibration is not possible. A method for utilizing the information from operational low gain PMTs was developed, and the basic strategy is as follows:
\begin{enumerate}
	\item The change in gain causes the effect of the 0.3 p.e. threshold on hit probability to change. The no-hit probability was expanded as follows:
	\begin{equation}
		P^{\prime}_{no-hit, i} = \left(1+\epsilon_1\mu_i+\epsilon_2\frac{\mu_i^2}{2!}+\epsilon_3\frac{\mu_i^3}{3!}\right)e^{-\lambda\mu_i}
	\end{equation}
	This model was originally a simple expansion of $P_{no-hit}$, but in the end was adjusted phenomologically to better reproduce real data. This adjustment is why an additional $e^{-\lambda\mu}$ appears in the model.
	\item The parameters $\epsilon_1, \epsilon_2, \epsilon_3,$ and $\lambda$ are estimated with actual data. The events satisfying the following selections are collected and the no-hit probability is calcualted for each expected charge. The expected charge of the i-th PMT $\mu_i$ is estimated using the vertex and total charge of the events that meet the following conditions.
	\begin{itemize}
		\item $r<6m$
		\item Not muons or events within 2 ms after muons
		\item Events with more than 120 17-inch PMT hits
		\item PMT waveforms that contain only 1 peak
	\end{itemize}
	Figure \ref{fig:nohit_prob} shows the result of fitting this adjusted no-hit probability model. THe fitting is performed run-by-run and for each PMT independently.
	\item Use the updated no-hit probability pdf in the event energy reconstruction in Equation \ref{eq:energy_likelihood}.
\end{enumerate}
\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.4]{no-hitprob.png}
	\caption{Fitting no hit probability to a low gain PMT against the expected charge $\mu$. The original model is shown with the blue line while the red line is the new model which agrees better with low-gain PMT data.}
	\label{fig:nohit_prob}
\end{figure}

Making use of the low-gain PMTs can improve energy resolution by up to 3\% \cite{karino_master}. Further analysis in this work uses energy reconstructed from the combination of normal and low-gain PMTs.

\subsection{Muon Reconstruction}
The selection and understanding of muons and muon-correlated neutrons are essential to multiple background rejections. This section describes the special selection criteria and reconstruction methods used for muons and neutrons.
\subsection*{Muon Selection Criteria}
The muon event selection criteria are as follows:
\begin{itemize}
	\item Total charge of 17-inch PMTs, $Q_{17}\geq 10000$ p.e.
	\item $Q_{17}\geq 500$ p.e. and the number of hit OD PMTs $\geq 9$.
\end{itemize}
The former criterion selects muons which go through the scintillator volumes of the detector. A total charge of 10,000 p.e. roughly corresponds to an event energy of 30 MeV, which exceeds the energy range of most physical analyses in KamLAND-ZEN. The second selection is for muons that only deposit energy in the outer buffer oil (clipping muons). Muons passing through the buffer oil volumes do not scintillate; as such, the 500 p.e. threshold in Cherenkov radiation roughly corresponds to about 40 MeV of energy deposition.

\subsection*{Cosmic Ray Muon Reconstruction}
Cosmic ray muon events form tracks as opposed to the point-like events caused by single decay events. The process is shown diagramattically in Figure \ref{fig:muon_reco}
\begin{enumerate}
	\item The ID PMT which detects the earliest light is identified. If the charge of this hit is low or isolated in time from the many other hits in the event, it is classed as a dark hit and ignored. A line is drawn from the earliest hit muon PMT and the center of the KamLAND detector. The intersection of this line and the outer balloon is marked as the temporary entrance point.
	\item The PMT whose charge is the largest is identified. The brightest hit PMT should be hit later than the earliest PMT and the neighbors of the earliest PMTs. A line is drawn from the brightest hit PMT and the center of the KamLAND detector. The intersection of this line and the outer balloon is marked as the temporary exit.
	\item The temporary track is defined as the line connecting the temporary entrance and exit. The temporary track is finally corrected by checking the correlation between the track length and the total charge.
	\item The reconstruction quality is evaluated by checking the following:
	\begin{itemize}
		\item Whether the earliest and the brightest PMTs can be identified
		\item Whether the mean hit time of PMTs around the entrance is earlier than the around the exit.
	\end{itemize}
	A "badness" parameter value is assigned to the reconstruction according to this evaluation. With this evaluation, around 15\% of muon candidates are regarded as badly reconstructed though they can still be used in muon-neutron pairing. Bad muon reconstruction is caused by ringing in the PMT signals, muon bundles, and stopped muons.
\end{enumerate}
The light yields in the muon events are estimated in \cite{karino_master}:
\begin{equation}
	\langle dQ_C/dX\rangle =28\pm 5\textbf{p.e./cm (Cherenkov muons)}
\end{equation}
\begin{equation}
	\langle dQ_S/dX\rangle =338\pm 12\textbf{p.e./cm (Scintillation muons)}
\end{equation}
\subsection{MoGURA Neutron Reconstruction}
\label{sec:mogura_neutron_reco}
Neutrons that are produced during cosmic ray spallation are best detected with the MoGURA DAQ due to the FBE's inability to handle the high after-pulse rate. After-pulsing is also present in MoGURA and needs to be rejected. An effective number of hits $N_s$ was introduced. The neutron reconstruction procedure is as follows:
\begin{enumerate}
	\item A 200 ns wide time window is opened. The vertex is reconstructed using LT Vertex with the hit information contained in this window.
	\item The times of flight to each PMT are calculated assuming the reconstructed vertex. Then the ToF-subtracted hit timing distribution is obtained.
	\item The obtained residual hit time distribution includes neutron capture 2.2 MeV gamma scintillation light and fake signals from after-pulses. To calculate the effective number of hits, $N_{in}$ and $N_{out}$, the number of hits in a 30 ns wide "ontime" window and a 170 ns wide "offtime" window respectively are counted. $N_s$ is then calculated as
	\begin{equation}
		N_s=N_{in}-N_{out}\times\frac{30\,\mathrm{ns}}{170\,\mathrm{ns}}
	\end{equation}
	\item The ontime window is shifted by 20 ns, the clock time of MoGDAQ, and step 3 is repeated.
	\item The 200 ns time window is shifted and steps 1-4 are repeated. The 200 ns window and 30 ns ontime window that maximize $N_s$ are found. The vertex given by the $N_s$-maximizing time windows is taken as the reconstructed neutron capture event vertex.
\end{enumerate}
\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.4]{neutron_reco.png}
	\caption{A neutron capture events hit times showing the contribution of fake after pulses and the time windows used to calculate $N_s$}
	\label{fig:neutron_reco}
\end{figure}
\subsection{Muon Neutron Correlation}
The neutron selection process outlined above contains many noise events, thus the sample is only used in background discrimination when coincident with muons. In particular, MoGURA data neutrons are used to improve the rejection of xenon spallation products. The procedure for selecting muon-neutron pairs is:
\begin{enumerate}
	\item Check the end unixtime of the previous KamDAQ run and the start unixtime of the current KamDAQ run.
	\item Collect the MoGURA runs that collected data during this gap. Muon events collected by MoGURA are used in the gaps between KamDAQ runs; during KamDAQ runs, muons collected with FBE data are used.
	\item The delayed coincidence analysis is done to select neutron candidate events in a short time period after muons. The first cuts applied are on $dT>2500\mu $ and $N_{s}=N_{in}-N_{out}<100$, these events are first removed. The subsequent MoGURA neutron selection criteria are outlined below.  
\end{enumerate}

The neutron selection in MoGURA is outlined in Figure \ref{fig:neutron_selection}. Two quantities are used, $dT$, the time delay between the neutron event and the previous muon, and $N_s$. From the 2D distribution, one sees that the event rate is higher in the short dT region due to noise and after-pulses. The $N_s$ values also tend to be small due to signal loss caused by baseline overshoot in the PMTs. The following criteria were chosen to select MoGURA neutrons:
\begin{itemize}
	\item $N_{total}=N_{in}+N_{out}>150$ (Number of hit requirement)
	\item $N_s>50 \wedge 10<dT<1200\mu s $ (reject after-pulses and accidental events)
	\item $!((N_s<dT(\mu s)+70\wedge 10<dT<20 \mu s )||N_s<-0.8\times dT(\mu s)+106\wedge20<dT<70\mu s)$
\end{itemize}
Figure \ref{fig:neutron_dT} shows the dT distribution of the MoGURA neutrons collected with the above criteria. The histogram is fitted to an exponential between 500 and 1000 $\mu s$, and the fitted function is extrapolated to the rest of the data range. The inefficiency of the neutron tag in the shorter dT period can be seen as the distribution turns off at low dT.

\begin{figure}[htb]
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.3]{mog_neutron.png}
        \caption{distribution showing the $dT$ dependence of $N_s$. The events above the red line are selected as MoGURA neutrons and used for background rejection \cite{takeuchi_phd}}
        \label{fig:neutron_selection}
    \end{minipage}
    \hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.3]{mog_neutron_dt.png}
        \caption{The $dT$ distribution of selected MoGURA neutrons. The fit to an exponential is performed between 500 and 1000 $\mu s$. \cite{takeuchi_phd}}
        \label{fig:neutron_dT}
    \end{minipage}
\end{figure}

\section{Event Selection}
Candidate \0nbb events must pass several event selections. The selections are separated into non-physical events and background cuts. In this section, we first describe the event selections used in this analysis. Then, the impact of these selections on signal inefficiency is discussed.

\subsection{Unphysical and Bad Quality Event Rejection}
\label{sec:badqualityevents}
Much of the data saved in the KamLAND DAQ systems includes "unphysical" events. Furthermore, many of the events associated with real physical processes are of poor quality. This section describes the criteria by which unphysical events and poor quality events are selected.
\begin{enumerate}
	\item Flasher PMT \\
	PMTs can occasionally emit light into the detector. These occurrences are called PMT flashers. There are multiple potential causes for a PMT to flash, such as the discharge of the dynodes and light emission from the epoxy around the breeder circuit. Figure \ref{fig:flasher} shows the typical event display of a flasher event. Such events have a distinct signature, as the PMT that flashes will have exceptionally high charge and cause a huge deposition of charge in the nearest PMTs as well. The PMT flasher selection criteria are as follows:
	\begin{itemize}
		\item Total charge of ID PMTs $ > 2500$ p.e.
		\item Maximum ID PMT charge/Total ID PMT Charge $ >60\%$
		\item Average charge of the neighboring PMTs $ > 20$ p.e.
	\end{itemize}
	\begin{figure}[htb!]
		\centering
        \includegraphics[scale=0.45]{flasher.png}
        \caption{Event display of a flasher event. The left shows the charge distribution. One PMT has an exceptionally large charge. Right shows the time distribution. It is relatively flat since the source is not scintillator. The hit timing of the flasher PMTs and its neighbors are very early.}
        \label{fig:flasher}
	\end{figure}
	\item Post muon events \\
	Cosmic ray muons deposit a large amount of energy into the detector. As a result, the detector behavior is unstable for a period afterwards. In particular, the detector suffers from a high fake event rate due to after-pulsing and the shift of baseline from overshoot. This instability causes not only a large amount of unphysical events, but also degrades the reconstructibility of real physical events. Thus, all events in the immediate 2 ms after cosmic ray muons are not used for the excited state analysis. However, these events may be used for other analyses such as spallation background estimation.
	\item Missing waveform events \\
	High rate after-pulsing from cosmic ray muons can also cause the ATWD to be busy and the DAQ system to get stuck. When the DAQ electronics are in this state, event waveforms cannot be recorded. These are referred to as "missing waveform" events. In these situations, the number of hit 17-inch PMTs within 125 ns after a trigger issue is recorded as "NsumMax". In properly recorded events, NsumMax should be proportional to the total number of 17-inch PMT hits (Nhit17). So the missing waveform events are identified by the ratio between NsumMax and Nhit17; the distributions of these two quantities are shown in Figure \ref{fig:missing_waveform}. Events tagged by this selection are removed from the excited state physics analysis. The exact selection criteria are as follows:
	\begin{itemize}
		\item Nhit17 $<$ NsumMax $\times 0.99-25$
		\item $dT$ after muon events $< 2$ ms (if NsumMax$<$1200)
		\item $dT$ after muon events $< 2$ s (if NsumMax$>$1200)
	\end{itemize}
	\begin{figure}[htb]
		\centering
        \includegraphics[scale=0.45]{missing_waveform.png}
        \caption{Nhit17 vs NHitSumMax distributions for all physics events (left) and Bi214-tagged events (right). The missing waveform cut inefficiency can be calculate from the Bi214-tagged events to be $< 0.01\%$}.
        \label{fig:missing_waveform}
	\end{figure}
	\item Post PPS trigger event \\
	The PPS trigger is a forced trigger issued once a second used for constant diagnostic monitoring of the detector. However, the PPS trigger has been found to cause an increase in electronics noise and DAQ trigger rate. Thus, events within 100 $\mu$s from the last PPS trigger are removed from the analysis.
	\item Badly reconstructed events \\
	The fit quality of an event's vertex reconstruction is the event's "Badness". The quantity is calculated using nine parameters that describe the deviation of an event's PMT hit and charge distribution from the expectation. A detailed explanation of the Badness calculation can be found in section 3.8.3 of \cite{}. These poorly reconstructed events mostly consist of noise events and pileup. The events with large Badness are removed from the analysis by the following energy-dependent threshold:
	\begin{equation}
		Badness < 25.0\times \exp(-4.5\times E_{vis} [\mathrm{MeV}]) + 3.1
	\end{equation}
	Figure \ref{fig:badness} shows the Badness distribution of all physical events and the Badness of $^{214}$Bi events selected by the delayed coincidence method.
	\begin{figure}[htb]
		\centering
        \includegraphics[scale=0.45]{badness.png}
        \caption{The badness distributions of all physics events (left) and Bi214-tagged events (right)}
        \label{fig:badness}
	\end{figure}
\end{enumerate}

\section{Background Rejection}
\subsection{Uranium/Thorium}
$^{214}$Bi and $^{212}$Bi are radioactive nuclei produced in the uranium and thorium decay series, respectively. They are among the dominant backgrounds in KamLAND-ZEN. These isotopes are introduced to the detector primarily by Uranium/Thorium contamination in the LS itself or on the surface of the inner balloon. There were also some $^{222}$Rn introduced when the XeLS was filled, this decayed with a half-life of 3.8 days, as such the $^{222}$Rn-related $^{214}$Bi decayed away in the early stages of KamLAND-ZEN 800. This time-dependence of the Radon background is accounted for in the physics analysis.
These Bismuth-decays are tagged in two ways: Delayed-coincidence veto and Double-pulse fitting.
\subsection*{Delayed Coincidence Veto}
Both $^{214}$Bi and $^{212}$Bi decays are quickly followed by the decays of $^{214}$Po/$^{212}$Po:

\begin{equation}
\begin{aligned}
&^{214}\mathrm{Bi}
\ \xrightarrow[\substack{19.9\,\mathrm{min},\ 99.98\%}]{\beta^-,\ Q=3.27\,\mathrm{MeV}}
\ ^{214}\mathrm{Po}
\ \xrightarrow[\substack{164.3\,\mu\mathrm{s}}]{\alpha,\ Q=7.83\,\mathrm{MeV}}
\ ^{210}\mathrm{Pb}
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
&^{212}\mathrm{Bi}
\ \xrightarrow[\substack{60.55\,\mathrm{min},\ 99.98\%}]{\beta^-,\ Q=2.25\,\mathrm{MeV}}
\ ^{212}\mathrm{Po}
\ \xrightarrow[\substack{0.299\,\mu\mathrm{s}}]{\alpha,\ Q=8.95\,\mathrm{MeV}}
\ ^{208}\mathrm{Pb}
\end{aligned}
\end{equation}
These decays are correlated in space and time in KamLAND, they can be vetoed by searching for this correlation. The criteria of the Bi-Po delayed coincidence analysis used in the KamLAND-ZEN analysis is as follows:
\begin{itemize}
	\item delayed energy : $0.2 < E_d < 1.3$ MeV
	\item distance between prompt and delayed vertices : $dR<170$ cm
	\item timing delay between prompt and delayed vertices : $dT<1.9$msec
\end{itemize}

Figure \ref{fig:BiPo214} shows the delayed-coincidence veto parameter distributions used in the $^{214}$Bi selection. Coincident pairs within 10 $\mu sec$ are excluded and set aside for the $^{212}$Bi selection, as $^{212}$Po has a much shorter half-life. The delayed energy deposition distribution, two peaks are found. THe lower energy peak corresponds to polonium decays in which some energy is deposited not in the liquid scintillator but in the mini-balloon nylon film.
\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.45]{bi214.png}
	\caption{The delayed coincidence selection parameters for $^{214}$Bi. The distributions of prompt energy, delayed energy, displacement, and delay time are shown. The blue shaded regions indicate the tagged events. Events with $dT<10\mu sec$ are set aside for $^{212}$Bi-Po selection.}
	\label{fig:BiPo214}
\end{figure}
Figure \ref{fig:BiPo212} shows the delayed-coincidence veto parameter distributions used in the $^{212}$Bi selection. For this selection, the timing selection is $dT<10\mu sec$. The veto efficiency of $^{214}$Bi-Po decays in the XeLS is $99.89\pm0.03\%$. The veto efficiency of $^{214}$Bi-Po decays in the balloon film is $48.9\pm9\%$. The lower veto efficiency is due to alpha decays depositing their energy in the film not the liquid scintillator. For $^{212}$Po decays that occur immediately after the initial $^{212}$Bi decay, they can be tagged by the double pulse fitter, which the next section describes.
\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.45]{bi212.png}
	\caption{The delayed coincidence selection parameters for $^{212}$Bi. The distributions of prompt energy, delayed energy, displacement, and delay time are shown. The yellow shaded regions indicate the tagged events. Only events with $dT<10\mu sec$ are used for this selection.}
	\label{fig:BiPo212}
\end{figure}
\subsection*{Pileup Events}
\label{sec:dpfit}
When the delay time, $dT$, between prompt and delayed events is small enough, Bi-Po sequential events can be stored as a single event in KamLAND's data acquisition window. In these cases, the delayed coincidence selection of two related events does not work. Such events are referred to as pile-up events. Since these pileup events contain the kinetic energy of the initial beta decay and the subsequent alpha decay, the combined deposited energy reaches beyond the \0nbb ROI and becomes an important background to reduce. The energy spectrum of these $^{212}$Bi-Po pile-up events is shown in Figure \ref{fig:BiPo_energy}.

A double pulse fit method has been developed to tag these pile-up events. The method simply searches for events with a hit-timing distribution indicative of two distinct energy depositions, or hit time peaks. The hit time distribution of an event is fitted with 2 reference waveforms by the following procedure:
\begin{enumerate}
	\item Construct a reference time profile.
	A reference waveform is made from the hit time profile of \2nbb-decay candidates. The events are selected with $1.4 < E_{vis}<1.6$ MeV and $radius < 157$ cm after \0nbb background vetoes. 
	\item Construct the hit timing profile of \0nbb decay candidates.
	The hit timing profile of the candidate events (events to be analyzed) is constructed. In the typical analyses and reconstructions, the time of the first hit on a PMT in an event is used. For this procedure, all hit times and hit charges (multi-hit) information is used to effectively separate the two peaks. 
	\item Fit the event's hit time profile
	The hit time profile is fitted by the reference waveform. The fit includes 4 parameters: $E_p$ (prompt signal energy), $T_p$ (prompt signal timing), $E_d$ (delayed signal energy), $\Delta T$ (time delay between prompt and delayed signals). Then a maximum likelihood optimization is performed. $\chi^2$ for the fit is defined as:
	\begin{equation}
		\chi^2 = 
		\begin{cases}
		2\sum_i \left\{ -(x_i-f_i) + x_i \log \frac{x_i}{f_i} \right\} & (n_i > 0) \\
		2\sum_i \left\{ -(x_i-f_i) \right\} & (n_i = 0)
		\end{cases}
		\hspace{2cm} (5.27)
	\end{equation}
	where $i, x_i$, and $f_i$ denote the $i$-th bin, the number of hit PMTs in the $i$-th bin and the expectation of the number of hits in the $i$-th bin, respectively. The time bins are 1 ns wide. $f_i$ can be calculated as the sum of a dark rate $D$ and the reference waveform $R(i)$ as:
	\begin{equation}
		F_i = |E_p|R(i-T_p)+|E_d|R(i-T_p-\Delta T) + D
	\end{equation}
	Here, the dark rate is a simple global dark rate taken from all the PMTs in the off-time period. The $\chi^2$ minimization is performed with MINUIT in the ROOT analysis framework. For all the $(T_p, \Delta T)$ pairs, $E_p$ and $E_d$ are floated, and the optimal four parameters are found.
	\item Correct the reconstructed energies
	$E_p$ and $E_d$ are used in the double-pulse fit to scale the reference waveform but they are not accurate individual pulse energy reconstructions. The energy reconstruction described in section \ref{sec:energy_reco} is more accurate. This "official" energy reconstruction is incorporated into the individual pulse energies by:
	\begin{equation}
		E_{p^\prime}=E_{vis}\times\frac{E_p}{E_p+E_d}
	\end{equation}
	\begin{equation}
		E_{d^\prime}=E_{vis}\times\frac{E_d}{E_p+E_d}
	\end{equation}
	Thus, the double-pulse fit provides the fraction of the total event energy, $E_{vis}$, to assign to the prompt and delayed signals.
	\item Candidate selection
	Finally, the pile-up tagged events are selected from the candidate events based on their $\Delta T$ and $E_{d^\prime}$. The selection criteria are determined using MC simulated events while limiting the \0nbb inefficiency to about 0.1\%. Figure \ref{fig:pileup_selection} shows the selection criteria over the MC distributions. The remaining $^{212}$Bi-Po events that pass pile-up are estimated to be $2.3\pm0.5\%$.
\end{enumerate}
\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.35]{dp_selection.png}
	\caption{The selection criteria over $E_d$ and $\Delta T$ used to tag pileup events. The distributions are of MC \0nbb (left) and $^{212}$Bi-Po (right). The regions enclosed by the red lines are vetoed as pileup events.}
	\label{fig:pileup_selection}
\end{figure}

\subsection{Antineutrinos}
The original physics objective of the KamLAND experiment was to observe electron anti-neutrinos produced in nuclear reactors, among other sources. The signature of anti-neutrinos in KamLAND and KamLAND-ZEN is inverse beta decay (IBD):
\begin{equation}
	\bar{\nu}_e+p\rightarrow e^+ + n
\end{equation}
IBD produces a neutron and a positron. The positron annihilates with an electron to produce gamma rays, 2 $\times$ 511 keV gammas. The neutron scatters in the detector until being captured by protons with an average capture time of $\tau=207\,\mu$s. This two-stage signal is ideal for tagging with the delayed coincidence method.

In the anti-neutrino analysis, IBD events were originally selected by a likelihood ratio selection; but in KLZ, a simple box cut analysis is applied. The IBD selection criteria are:
\begin{itemize}
	\item delayed energy : $E_d>1.5$ MeV
	\item distance between prompt and delayed vertices : $dR<200$ cm
	\item timing correlation between prompt and delayed vertices : $dT < 1.0$msec
\end{itemize}
In the wake of the 2011 Tohoku earthquake and reactor meltdown, the Japanese nuclear power plants were turned off and the rate of anti-neutrino events are less than 0.2 event/day within $r<550$ cm. The efficiency of the IBD box cuts are 99.14\%. Thus, the remaining potential IBD background to the KLZ analyses are negligible.
\subsection{Short-lived Spallation Products}
High energy cosmic ray muons can break apart nuclei in the detector material into lighter nuclei and produce secondary particles. The spallation products of $^{12}$C are one of the dominant backgrounds in the $2n\beta\beta^*$ search. Multiple tagging methods have been developedfor identifying so-called short-lived spallation products.
\subsection*{$^{12}$B Veto}
The $^{12}$B $\beta$-decay has a large contribution to the background in the \0nbb ROI. The rate of muons is $\approxeq 3$ Hz in the KamLAND detector thus $^{12}$B can be removed with a veto after muons. In this analysis, a 150 msec window, corresponding to 5 times the livetime of $^{12}$B, is vetoed and counted as detector deadtime.

\subsection*{MoGURA Neutron Veto}
\label{sec:mogura_neutron_veto}
Neutrons are also ejected during the muon spallation process. These neutrons are correlated with the spallation products. The MoGURA DAQ system is used to observe neutron captures that occur shortly after cosmic-ray muons. The correlation with these MoGURA neutrons can be used to tag other short-lived spallation products, mainly $^{10}$C, $^{6}$He, and $^{8}$Li. The selection of the MoGURA neutron veto is as follows:
\begin{itemize}
	\item distance between candidate decay and spallation neutron : $dR<160$ cm
	\item timing correlation : $dT < $ 180 s (about 5 times the lifetime of $^{10}$C)
\end{itemize}

\subsection*{$^{137}$Xe Veto}
Neutrons capturing onto $^{136}$Xe nuclei form $^{137}$Xe. The Q-value of $^{137}$Xe $\beta^-$-decay is 4.2 MeV with a half-life of 3.82 minutes. This decay can be tagged by the triple coincidence of a muon, the neutron capture, and the beta decay itself. The selection criteria is as follows:
\begin{itemize}
	\item distance between candidate decay and spallation neutron : $dR<160$ cm
	\item timing correlation : $dT < $ 1,620 sec
\end{itemize}

Since the neutron capture gamma on $^{137}$Xe has a gamma energy about 4 MeV, higher than captures on protons, the neutron selection criteria is adjusted slightly. The $^{137}$Xe neutrons are required to have $N_s$ values above 240. A schematic of spallation veto of Carbon and Xenon is shown in Figure \ref{fig:spall_veto}.
\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.35]{spall_veto.png}
	\caption{A schematic of spallation veto with MoGURA neutrons}
	\label{fig:spall_veto}
\end{figure}

\subsection{Shower Veto}
As a cosmic-ray muon passes through the detector, it does not interact with uniform probability along its path. There is a point where it deposits the most energy and spallates the most nuclei. Secondary decay products are correlated with this position. The shower veto identifies this location of high energy deposition and correlates background with this position.
\subsection*{PDF$(dE/dX,dL)$}
$dE/dX$, the distribution of energy deposition along muon tracks, and $dL$, the distance from the muon track to the candidate event, are correlated to the decay candidates. A two dimensional PDF$(dE/dX,dL)$ is constructed from muon data vy the following procedure:
\begin{enumerate}
	\item The cosmic ray muons are selected and their tracks reconstructed.
	\item The time when the muon entered the ID $T_0$ is calculated.
	\item THe distance between the muon ID entrance and the generated point $L$ is calculated for each photon. Figure \ref{fig:showerphoton} shows the schematic of photon generation from cosmic muons. 
	\begin{equation}
		x^2_2=r^2+s^2_1-2x_1r\cos\theta
	\end{equation}
	\begin{equation}
		x_1+nx_2=c(t-t_0)
	\end{equation}
	Here, $n$ is the refractive index of the liquid scintillator. These equations are solved for $x_1$, the distance L can be obtained. For general analysis and reconstruction, only the PMT hit times of the first photons are used. However, for this calculation, multiple photons for each PMT are considered, referred to as multiTQ analysis. $L$ is calculated for each incident photon on each PMT. 
\end{enumerate}

\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.35]{showerphoton.png}
	\caption{$dE/dx$ reconstruction.}
	\label{fig:showerphoton}
\end{figure}

$^{12}$B is used for estimating the PDF$(dE/dX,dL)$ as it can be easily tagged by the $dT$ selection after muon events. Figure \ref{fig:shower_pdf} shows an example of a $dE/dX$ calculation result. The accidental event likelihood can also be calculated using the off-time events. The spallation backgrounds are rejected by calculating a likelihood ratio value of spallation vs accidental. THe log-likelihood ratio threshold used is -1.8, events with spallation-accidental logarithmic ratios below this value are classified as spallation backgrounds and rejected.

\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.75]{shower_dedx.png}
	\caption{An example of a calculated $dE/dx$. The muon's largest energy deposit can be seen at $L_{long}=600$ cm.}
	\label{fig:shower_pdf}
\end{figure}

\subsection{Xenon Spallation Products}
The spallation products of Xenon are an important background to the \0nbb search. For the $2\nu\beta\beta^*$ search, they obscure the endpoint of the \2nbb spectrum. Rejecting this background is important to both double-beta physics searches. However, these heavier nuclei can have half-lives of hours or longer, much longer than their Carbon-spallation counterparts; therefore, the MoGURA neutron veto is ineffective against these. 

For these backgrounds, a likelihood selection has been developed specifically for these "long-lived" spallation products. The method involves constructing PDFs of $dT$ (time since muon), $dR$ (spatial distance from the nearest neutron created by each muon), and $ENN$ (Effective Number of Neutrons).

$^{136}$Xe spallation is characterized by the production of many free neutrons; these post-muon neutron captures are useful in identifying likely long-lived spallation isotopes. However, there is a high rate of accidental neutrons and unphysical detector noise that can be attributed to neutrons. The $ENN$, Effective Number of Neutrons, was developed to define a weighted counting of neutrons by who likely they are to be related to a given event. Each neutron following a muon is assigned a weight based on the spatial distributions of neutron captures from spallation products and accidentals. Figure \ref{fig:ENN_dR} shows the weight over $dR$.
\begin{equation}
	\label{eq:ENN}
	ENN = \sum_{neutrons}\frac{PDF_{spl.}(dR)}{PDF_{spl.}(dR)+PDF_{acc.}(dR)}
\end{equation}
The PDFs are the probability distributions of $dR$ which is the spatial distance between a candidate event and a neutron event. Spallation products and neutrons that originate from the same muon have a spatial correlation. The PDFs are shown in Figure \ref{fig:spall_dR}. $PDF_{spl.}(dR)$ is modeled with an exponentially modified Gaussian distribution, while $PDF_{acc.}(dR)$ is simply quadratic, assuming uniform distribution in space of uncorrelated events. The exponentially modified Gaussian function features 3 free parameters:
\begin{equation}
	f(x;\mu,\sigma,\lambda)=\frac{\lambda}{2}exp\left(\frac{\lambda}{2}(\mu+\lambda\sigma^2-2x)\right)erfc\left(\frac{\mu+\lambda\sigma^2-x}{\sqrt{2}\sigma}\right)
\end{equation}

$erfc(x)$ is the complementary error function defined as $erfc(x)=1-erf(x)$. The free parameters are determined using $^{10}$C data. The sum in equation \ref{eq:ENN} is over all the neutrons in a short window after a muon, it is a quantity assigned to each muon.

\begin{figure}[htbp]
	\centering
	\subcaptionbox{dR distribution for neutrons associated with spallation products and accidental neutrons\label{fig:spall_dR}}[0.45\textwidth]{\includegraphics[width=0.45\textwidth]{spall_dR.png}}
	\hfill
	\subcaptionbox{Long-lived events\label{fig:longlived}}[0.45\textwidth]{\includegraphics[width=0.45\textwidth]{ENN_dR.png}}
	\caption{Weighting factor function for ENN.}
	\label{fig:ENN_dR}
\end{figure}

\subsection*{Long-Lived Spallation Likelihood}
Long-lived spallation backgrounds are handled not by rejecting them from the analysis outright, but by separating long-lived spallation candidates into a separate spectrum to be fitted alongside the residual events. The long-lived data (LD) and the singles data (SD) are separated by calculating a likelihood ratio threshold. The likelihood ratio is defined as:
\begin{equation}
	R_L=\frac{L_{spl.}}{L_{acc.}+L_{spl.}}
\end{equation}
where $L_{acc.}$ and $L_{spl.}$ are the likelihood functions of accidental events and long-lived spallation backgrounds.

The spallation product likelihood is constructed from:
\begin{equation}
	L_{spl.}(dR_{near}, ENN, dT)=\sum_{spl. products}PDF(dT)\times PDF(dR_{near}, ENN)
\end{equation}
where the sum is calculated for all spallation isotopes listed in Table \ref{tab:spallproducts}. There is an implicit assumption that the time-component of the likelihood and the $PDF(dR, ENN)$ are independent. $L_{acc}$ is assumed to be uniform in time and is defined as:
\begin{equation}
	L_{acc.}(dR, ENN, dT) = PDF(dR, ENN)
\end{equation}

The likelihoods are constructed from FLUKA simulations and real muon data in KamLAND-ZEN. The long-lived likelihood uses FLUKA simulations of spallation product production and neutron ejection. The accidental likelihood uses events that occur after muons to get a data-informed $dR$ and $ENN$ distribution. Figure \ref{fig:likelihood_dR_ENN} shows the 2D profiles of the two likelihood functions. The differences are clear, in particular that the spallation products are more likely to have higher $ENN$ and lower $dR$. In order to avoid dividing by a joint 0 likelihood in the likelihood ratio, bin-smoothing is done to both likelihoods to extend them into the full range.

\begin{figure}[htbp]
	\centering
	\subcaptionbox{Spallation Products\label{fig:spall_dR_ENN}}[0.45\textwidth]{
		\includegraphics[width=0.45\textwidth]{spall_dR_ENN.png}}
	\hfill
	\subcaptionbox{Accidental events\label{fig:acc_dR_ENN}}[0.45\textwidth]{
		\includegraphics[width=0.45\textwidth]{acc_dR_ENN.png}}
	\caption{Weighting factor function for ENN.}
	\label{fig:likelihood_dR_ENN}
\end{figure}

Figure \ref{fig:LHR} shows the distributions of log-likelihood ratios, calculated from our PDFs from toy-MC trials. $10^6$ events are generated for each PDF. The log-likelihood ratio is $\log_{10}\left(1-\frac{L_{acc}}{L_{acc}+L_{spl}}\right)$, the logarithm is taken to bring all events into a similar range. With this definition, a smaller likelihood ratio or LHR means a higher likelihood of being a long-lived spallation product and a larger LHR indicates a higher likelihood of being an accidental event.
\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.35]{LHR.png}
	\caption{Log-Likelihood ratio distributions generated from the PDFs by a ToyMC study. A clear separation between the distributions can be seen.}
	\label{fig:LHR}
\end{figure}

\subsection*{Figure of Merit}
To separate the data events into SD/LD datasets a threshold on the LHR variable must be determined. This threshold is determined by a Figure of Merit calculation (FOM).
\begin{equation}
	\text{FOM}=\frac{S(t)}{\sqrt{s(t)+B(t)}}
\end{equation}
$S(t)$ and $B(t)$ are the integrated LHR distributions above threshold, $t$, of signal and background generated by the toyMC study above. Due to inconsistent run conditions, including MoGURA livetime and efficiency, the datasets is separated into 3 time periods and a threshold is independently determined for each time period.
\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.35]{FOM.png}
	\caption{Long-lived spallation veto Figure of Merit}
	\label{fig:FOM}
\end{figure}

\subsection*{Veto Efficiency}
The veto efficiency of the long-lived spallation veto is estimated for each spallation background in Table \ref{tab:spall_isotopes} using ToyMC informed by FLUKA simulations. FLUKA simulated isotope production and neutron correlation is convolved with our measured KamLAND-ZEN energy and vertex resolution. The tagging procedure using the above PDFs and likelihood ratio is applied to estimate the veto efficiency.

Furthermore, some of the other background rejection methods will simultaneously help reject some of the long-lived spallation backgrounds. Namely, the pileup veto and MoGURA neutron veto reject some spallation backgrounds; the effects of those vetoes are described as:
\begin{itemize}
	\item Pileup veto:
	The double pulse fitter for pileup veto is sensitive to the o-Ps (ortho-positronium decays) of $\beta^+$ decaying isotopes. The efficiency of the double-pulse fitter is estimated for each isotope using the lifetime and production rate of o-Ps in \cite{positrons_in_kamland}. Averaging over the isotopes weighted by production, the efficiency is 4.01\%.
	\item MoGURA neutron veto:
	Long-lived spallation products that happen to decay in the 160 s time window of the MoGURA neutron veto are tagged. FLUKA and GEANT4 simulations inform an averaged efficiency of 6.6\%.
\end{itemize}

\subsection*{Uncertainties}
Table \ref{tab:fluka_error} lists the estimated systeamtic errors of the FLUKA simulation of long-lived spallation isotope production. The most significant of these systematic errors is the uncertainty in the FLUKA spallation simulation. In the abscence of dedicated cosmic-ray muon induced xenon-spallation measurements, checking the validity of FLUKA's spallation simulation is difficult. The results of 2 beam experiments are taken into account. The first is an experiment where a 490 GeV $\mu^+$ beam on gaseous xenon reports the production of charged hadrons \cite{mubeam_fluka}. Test FLUKA simulations can reproduce their measurements.

\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.35]{distortion.png}
	\caption{The comparison between FLUKA simulation and Xenon beam experiment data. Red line show the diﬀerence from \cite{xenonbeam_500MeV}. Blue line shows the diﬀerence from \cite{xenonbeam_1GeV}. The red line is adopted as the error and considered in the spectrum ﬁt since the deviation in \0nbb ROI is larger}
	\label{fig:distortion}
\end{figure}

Another expriment measured the cross section of a $^{136}$Xe beam on a 1 $cm^3$ liquid-hydrogen target. The incident energy per nucleon used was 500 MeV \cite{xenonbeam_500MeV} and 1 GeV \cite{xenonbeam_1GeV}. The comparison of our FLUKA derived long-lived spallation decay spectrum and these beam measurements are shown in Figure \ref{fig:distortion}. 


\begin{table}[h]
	\centering
	\begin{tabular}{lc}
		\hline
		& Error \\
		\hline
		Time-bin dependence & 2.9\% \\
		Neutron detection efficiency & 2.73\% \\
		Comparison between FLUKA and beam experiments & 7.5\% \\
		Energy resolution of MoGURA & 5.67\% \\
		\hline
	\end{tabular}
	\caption{Errors of FLUKA}
	\label{tab:fluka_error}
\end{table}

\subsection*{Spectrum Distortion}
The discrepancy shown in Figure \ref{fig:distortion} indicates a potential mismodeling of the long-lived spallation background energy distribution. This uncertainty is implemented in the spectral fit, by introducing a distortion parameter that varies the model of the long-lived spallation background, based on this discrepancy with experiment.
\subsection{Signal Inefficiency}
The effect of all these background vetoes on the \0nbb signal needs to be understood. This signal inefficiency is determined by calculating the livetime of the analysis. 

The livetime calculation is done by applying the same event selections to toy MC events distributed uniformly in time and space. Real detector data is used for the Muon-neutron pairs, delayed coincidence analysis, 1PPS trigger event, and missing waveform events. Livetime is the fraction of runtime leftover after event selection.
\begin{equation}
	\text{Livetime} = \frac{\text{\# of Toy MC events after event selection}}{\text{\# of generated Toy MC events}}\times \text{Runtime}
\end{equation}
The \2nbb$^*$ spectral fit is performed over long-lived spallation enriched and depleted events simultaneously. A later section describes this selection. For proper relative normalization, the livetime is calculated for both long-lived enriched and depleted samples. The deadtime of the KLZ physics analysis is listed in Table \ref{tab:deadtime}.
\begin{table}[htb]
\centering
\caption{Summary of the deadtime}
\label{tab:deadtime}
\begin{tabular}{|l|c|}
\hline
\textbf{Event selection} & \textbf{Deadtime ratio [\%]} \\ \hline
Spallation veto & 14.64 \\ \hline
MoGURA neutron veto & 4.91 \\ \hline
$^{8}$Xe veto & 1.33 \\ \hline
Shower veto & 7.37 \\ \hline
$^{12}$B veto & 3.11 \\ \hline
Xe spallation veto & 8.56 \\ \hline
\begin{tabular}[c]{@{}l@{}}Detector deadtime veto\\(post PPS, after muons and missing waveforms)\end{tabular} & 9.47 \\ \hline
Hardware related & 0.0078 \\ \hline
Delayed coincidence Ra veto & 0.0013 \\ \hline
Delayed coincidence Reactor veto & 0.0010 \\ \hline
\textbf{Total} & \textbf{29.52} \\ \hline
\end{tabular}
\end{table}

The signal inefficiency of the double-pulse fitter and vertex Badness are omitted from Table \ref{tab:deadtime}. This is due to the Toy MC livetime study not being done at the PMT hit level. Sections \ref{sec:dpfit} \& \ref{sec:badqualityevents} show that the signal inefficiency from these selections is negligible.

The signal inefficiency of the double-pulse fitter and vertex Badness are omitted from table \ref{tab:deadtime}. This is due to the Toy MC livetime study not being done at the PMT hit level. Sections \ref{sec:dpfit} \& \ref{sec:badqualityevents} show that the signal inefficiency from these selections are negligible.
